{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMbQ59EiMNS0liiLn5jVQtl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iraidaantropova/data-processing-method/blob/main/%D0%9F%D0%A05_%D0%A1%D0%B8%D0%BD%D0%B8%D1%86%D0%B0%D0%98.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание\n",
        "\n",
        "1 вариант\n",
        "\n",
        "1. Доработать паука в имеющимся проекте, чтобы он формировал item по структуре\n",
        "\n",
        "-наименование вакансии\n",
        "\n",
        "-зарплата от\n",
        "\n",
        "-зарплата до\n",
        "\n",
        "-ссылку на саму вакансию\n",
        "\n",
        "2. создать в имеющимся проекте второго паука по сбору вакансий с superjob\n",
        "\n",
        "\n",
        "2 вариант\n",
        "\n",
        "1. Создать пауков по сбору данных о книгах с сайтов labirint.ru и/или book24.ru\n",
        "\n",
        "2. Каждый паук должен собирать:\n",
        "\n",
        "- ссылку на книгу\n",
        "\n",
        "- наименование книги\n",
        "\n",
        "- автор\n",
        "\n",
        "- основную цену\n",
        "\n",
        "- рейтинг книги\n",
        "\n",
        "3. собранная информация должна складываться в базу данных"
      ],
      "metadata": {
        "id": "86rSiW7muhU_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вариант 2"
      ],
      "metadata": {
        "id": "BZfCvLok0Cz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scrapy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5W4NuTiPM7Z",
        "outputId": "d4078e36-b111-44e1-a1a1-ca7297187b41"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scrapy in /usr/local/lib/python3.8/dist-packages (2.7.1)\n",
            "Requirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.8/dist-packages (from scrapy) (39.0.0)\n",
            "Requirement already satisfied: Twisted>=18.9.0 in /usr/local/lib/python3.8/dist-packages (from scrapy) (22.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from scrapy) (57.4.0)\n",
            "Requirement already satisfied: queuelib>=1.4.2 in /usr/local/lib/python3.8/dist-packages (from scrapy) (1.6.2)\n",
            "Requirement already satisfied: w3lib>=1.17.0 in /usr/local/lib/python3.8/dist-packages (from scrapy) (2.1.1)\n",
            "Requirement already satisfied: zope.interface>=5.1.0 in /usr/local/lib/python3.8/dist-packages (from scrapy) (5.5.2)\n",
            "Requirement already satisfied: itemadapter>=0.1.0 in /usr/local/lib/python3.8/dist-packages (from scrapy) (0.7.0)\n",
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.8/dist-packages (from scrapy) (3.4.0)\n",
            "Requirement already satisfied: itemloaders>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from scrapy) (1.0.6)\n",
            "Requirement already satisfied: protego>=0.1.15 in /usr/local/lib/python3.8/dist-packages (from scrapy) (0.2.1)\n",
            "Requirement already satisfied: pyOpenSSL>=21.0.0 in /usr/local/lib/python3.8/dist-packages (from scrapy) (23.0.0)\n",
            "Requirement already satisfied: PyDispatcher>=2.0.5 in /usr/local/lib/python3.8/dist-packages (from scrapy) (2.0.6)\n",
            "Requirement already satisfied: service-identity>=18.1.0 in /usr/local/lib/python3.8/dist-packages (from scrapy) (21.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from scrapy) (21.3)\n",
            "Requirement already satisfied: cssselect>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from scrapy) (1.2.0)\n",
            "Requirement already satisfied: lxml>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from scrapy) (4.9.2)\n",
            "Requirement already satisfied: parsel>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from scrapy) (1.7.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.8/dist-packages (from cryptography>=3.3->scrapy) (1.15.1)\n",
            "Requirement already satisfied: jmespath>=0.9.5 in /usr/local/lib/python3.8/dist-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from protego>=0.1.15->scrapy) (1.15.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.8/dist-packages (from service-identity>=18.1.0->scrapy) (22.2.0)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.8/dist-packages (from service-identity>=18.1.0->scrapy) (0.2.8)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.8/dist-packages (from service-identity>=18.1.0->scrapy) (0.4.8)\n",
            "Requirement already satisfied: Automat>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from Twisted>=18.9.0->scrapy) (22.10.0)\n",
            "Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.8/dist-packages (from Twisted>=18.9.0->scrapy) (21.0.0)\n",
            "Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.8/dist-packages (from Twisted>=18.9.0->scrapy) (15.1.0)\n",
            "Requirement already satisfied: incremental>=21.3.0 in /usr/local/lib/python3.8/dist-packages (from Twisted>=18.9.0->scrapy) (22.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.8/dist-packages (from Twisted>=18.9.0->scrapy) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->scrapy) (3.0.9)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.8/dist-packages (from tldextract->scrapy) (1.5.1)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.8/dist-packages (from tldextract->scrapy) (2.10)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from tldextract->scrapy) (2.25.1)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.8/dist-packages (from tldextract->scrapy) (3.9.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.12->cryptography>=3.3->scrapy) (2.21)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.1.0->tldextract->scrapy) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.1.0->tldextract->scrapy) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scrapy\n",
        "from scrapy.http import HtmlResponse\n",
        "from scrapy.crawler import CrawlerProcess"
      ],
      "metadata": {
        "id": "6GMsVz0sO9vS"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BookparserLabirintItem(scrapy.Item):\n",
        "    \"Bookparser item\"\n",
        "    title = scrapy.Field()\n",
        "    link = scrapy.Field()\n",
        "    author = scrapy.Field()\n",
        "    own_price = scrapy.Field()\n",
        "    sale_price = scrapy.Field()\n",
        "    rating = scrapy.Field()"
      ],
      "metadata": {
        "id": "xs-EYVWhQU64"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LabirintruSpider(scrapy.Spider):\n",
        "      name = 'labirintru'\n",
        "      allowed_domains = ['labirint.ru'] \n",
        "      start_urls = ['https://www.labirint.ru/']\n"
      ],
      "metadata": {
        "id": "uf4QciA5RdhO"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse (self, response):\n",
        "    links = response.xpath('//a[@class=\"product-title-link\"]/@href').extract()\n",
        "    for link in links:\n",
        "        yield response.follow('https://www.labirint.ru' + link, callback=self.parse_item)\n",
        "    next_page = response.xpath('//a[@class=\"pagination-next__text\"]/@href').extract_first()\n",
        "    if next_page:\n",
        "        yield response.follow('https://www.labirint.ru/novelty/' + next_page, callback=self.parse)"
      ],
      "metadata": {
        "id": "LD6k5I-1TrN1"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_item(self, response:HtmlResponse):\n",
        "        title = response.xpath('//h1/text()').extract_first()\n",
        "        link = response.url\n",
        "        author = response.xpath('//a[@data-event-label=\"author\"]/text()').extract_first()\n",
        "        own_price = response.xpath('//span[@class=\"buying-price-val-number\"]/text()').extract_first()\n",
        "        if not own_price:\n",
        "            own_price = response.xpath('//span[@class=\"buying-priceold-val-number\"]/text()').extract_first()\n",
        "        sale_price = response.xpath('//span[@class=\"buying-pricenew-val-number\"]/text()').extract_first()\n",
        "        rating = response.xpath('//div[@id=\"rate\"]/text').extract_first()\n",
        "\n",
        "        yield BookparserLabirintItem(title=title, link=link, author=author, own_price=own_price, sale_price=sale_price,\n",
        "                                     rating=rating)"
      ],
      "metadata": {
        "id": "Mz5cL9B2VMQ6"
      },
      "execution_count": 50,
      "outputs": []
    }
  ]
}